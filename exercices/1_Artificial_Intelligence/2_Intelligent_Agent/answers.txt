Link : https://aimacode.github.io/aima-exercises/agents-exercises/

1)Suppose that the performance measure is concerned with just the first T time steps of the environment and ignores everything thereafter. Show that a rational agent’s action may depend not just on the state of the environment but also on the time step it has reached.

For the example of the vacuum-cleaner, we want the robot to remember it cleaned one case few seconds ago or it will keep looking for dirty place in loop.

2)Let us examine the rationality of various vacuum-cleaner agent functions.
1. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed rational under the assumptions listed on page 
2. Describe a rational agent function for the case in which each movement costs one point. Does the corresponding agent program require internal state?
3. Discuss possible agent designs for the cases in which clean squares can become dirty and the geography of the environment is unknown. Does it make sense for the agent to learn from its experience in these cases? If so, what should it learn? If not, why not?

1. The agent described by figure 2.3 is rational because it does the right thing, its performance is maximal. It is not the optimal solution thought because it never stops cleaning.
2. A simple way to do that is to add a timer, after cleaning the room it waits 1 hour before checking the room again.
3. It makes sense because it could build a representation of its environment and therefore it could find the optimal path to visit the whole room. It could also figure out the rate at which the floor gets dirty.

3)Write an essay on the relationship between evolution and one or more of autonomy, intelligence, and learning.

Evolution used to be about random mutations, now it is a lot about learning and sharing informations. Intelligence is the capacity to adapt, we need to learn if we want to adapt and thus, evoluate. We sometimes learn by imitating, but we mostly learn autonomously since we are born. Those terms are closely linked together.

4)For each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.

1. An agent that senses only partial information about the state cannot be perfectly rational. FALSE
2. There exist task environments in which no pure reflex agent can behave rationally. TRUE
3. There exists a task environment in which every agent is rational. TRUE
4. The input to an agent program is the same as the input to the agent function. FALSE
5. Every agent function is implementable by some program/machine combination. FALSE
6. Suppose an agent selects its action uniformly at random from the set of possible actions. There exists a deterministic task environment in which this agent is rational. TRUE
7. It is possible for a given agent to be perfectly rational in two distinct task environments. TRUE
8. Every agent is rational in an unobservable environment. FALSE
9. A perfectly rational poker-playing agent never loses. FALSE

5)For each of the following activities, give a PEAS description of the task environment and characterize it in terms of the properties listed in Section 

- Playing soccer.
Perf : max number of goals, min number of goals of enemy team, min number of faults...
Env : soccer field, ally players, enemy players, referee...
Actuators : agent's body
Sensors : visual sensors, speedometer...
- Shopping for used AI books on the Internet.
Perf : max books quality, min price...
Env : internet
Actuators : ability to buy books
Sensors : website's content
- Bidding on an item at an auction.
Perf : min price, max items, max items quality...
Env : bidders
Actuators : ability to buy items
Sensors : price, visual sensors...

7)Define in your own words the following terms: agent, agent function, agent program, rationality, autonomy, reflex agent, model-based agent, goal-based agent, utility-based agent, learning agent.

Agent : An agent react with its actuators depending its sensors
Agent function : The agent function is the process trying to maximize the agent performance
Agent program : The agent program implements the agent function
Rationality : A rational agent has to do the right thing
Autonomy : An autonomous agent doesn't need controls from the outside to work
Reflex agent : A reflex agent reacts depending a signal from its sensors
Model-based agent : A model-based agent reacts depending a signal from its sensors but it also has a representation of the world state
Goal-based agent : A goal-based agent can make a decision depending its goal when the signal from its sensors is ambiguous
Utility-based agent : An utility-based agent does have a goal, but the goal is not a boolean, it can be a value which adds flexibility
Learning agent : A learning agent update its performance computation and its component continuously

8)This exercise explores the differences between agent functions and agent programs.

1. Can there be more than one agent program that implements a given agent function? Give an example, or show why one is not possible. 
TRUE, vacuum can clean depending vision sensors, but also with other sensors. 
2. Are there agent functions that cannot be implemented by any agent program?
TRUE
3. Given a fixed machine architecture, does each agent program implement exactly one agent function?
FALSE
4. Given an architecture with n bits of storage, how many different possible agent programs are there?
2^n
5. Suppose we keep the agent program fixed but speed up the machine by a factor of two. Does that change the agent function?
FALSE
9)Write pseudocode agent programs for the goal-based and utility-based agents.


10)Consider a simple thermostat that turns on a furnace when the temperature is at least 3 degrees below the setting, and turns off a furnace when the temperature is at least 3 degrees above the setting. Is a thermostat an instance of a simple reflex agent, a model-based reflex agent, or a goal-based agent?


11)Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.8 and specified on page . Your implementation should be modular so that the sensors, actuators, and environment characteristics (size, shape, dirt placement, etc.) can be changed easily. (Note: for some choices of programming language and operating system there are already implementations in the online code repository.)


12)Implement a simple reflex agent for the vacuum environment in Exercise 2.10. Run the environment with this agent for all possible initial dirt configurations and agent locations. Record the performance score for each configuration and the overall average score.


13)Consider a modified version of the vacuum environment in Exercise 2.10, in which the agent is penalized one point for each movement.
1. Can a simple reflex agent be perfectly rational for this environment? Explain.
2. What about a reflex agent with state? Design such an agent.
3. How do your answers to 1 and 2 change if the agent’s percepts give it the clean/dirty status of every square in the environment?


14)Consider a modified version of the vacuum environment in Exercise 2.10, in which the geography of the environment—its extent, boundaries, and obstacles—is unknown, as is the initial dirt configuration. (The agent can go Up and Down as well as Left and Right.)
1. Can a simple reflex agent be perfectly rational for this environment? Explain.
2. Can a simple reflex agent with a randomized agent function outperform a simple reflex agent? Design such an agent and measure its performance on several environments.
3. Can you design an environment in which your randomized agent will perform poorly? Show your results.
4. Can a reflex agent with state outperform a simple reflex agent? Design such an agent and measure its performance on several environments. Can you design a rational agent of this type?


15)Repeat Exercise 2.13 for the case in which the location sensor is replaced with a “bump” sensor that detects the agent’s attempts to move into an obstacle or to cross the boundaries of the environment. Suppose the bump sensor stops working; how should the agent behave?


16)The vacuum environments in the preceding exercises have all been deterministic. Discuss possible agent programs for each of the following stochastic versions:
1. Murphy’s law: twenty-five percent of the time, the Suck action fails to clean the floor if it is dirty and deposits dirt onto the floor if the floor is clean. How is your agent program affected if the dirt sensor gives the wrong answer 10% of the time?
2. Small children: At each time step, each clean square has a 10% chance of becoming dirty. Can you come up with a rational agent design for this case?

